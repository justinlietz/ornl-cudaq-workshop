{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- Multi-QPU (nvidia-mqpu)\n",
    "\n",
    "The `nvidia-mqpu` target is useful for distributing separate quantum circuits to individual GPUs on a single host machine. \n",
    "\n",
    "![img](./circuit-mqpu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: QML\n",
    "\n",
    "#### Example with `sample` algorithmic primitives\n",
    "![img](./RBM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of QPUs: 5\n",
      "{ 10110111:8 00010111:19 11100111:7 00000111:2 10011111:114 10111011:8 11011111:17 10011011:64 01011101:147 11110111:554 11011101:399 00101001:13 10101111:2 10111001:175 01101011:1 10011010:225 10110011:7 01000100:2 11100110:40 01111111:829 01001010:1 00111110:38 10110110:17 11111111:987 01101110:21 11001110:1 00111111:8 00000011:1 00011110:161 01000000:1 01000001:1 01101010:13 01001001:1 11011011:9 00010110:88 11111011:565 01010101:81 10100111:5 01101111:4 11110011:327 00101111:3 10110101:161 11011001:213 10110000:321 01011111:5 10111111:12 01001101:1 10011110:434 00011111:38 10110010:18 10110100:517 01001100:2 10110001:94 11101110:58 11101111:14 10111100:955 10111010:21 10111110:30 11001100:27 11001101:6 00111100:879 00111101:226 01000101:2 10111101:235 10111000:530 10001111:49 00001111:4 10100101:60 00101101:25 01010111:3 10101101:94 }\n",
      "\n",
      "{ 10110111:8 00010111:21 11100111:6 00000111:3 10011111:96 10111011:6 11011111:19 10011011:87 01011101:140 11110111:548 00001011:7 11100110:33 01111111:858 10110011:4 00111110:37 10110110:21 11111111:944 01101110:25 11001110:2 00111111:9 00000011:1 00011110:157 01000000:2 11011011:13 01001001:1 00010110:92 01001000:3 10110000:328 01001101:1 10011010:236 01101011:2 10111111:13 01011111:6 11111011:537 01010101:87 10100111:3 10100101:40 00001111:5 01101111:5 11110011:325 10101111:3 10111001:139 11011001:219 01101010:14 10001111:41 01000101:1 10111101:236 10111000:557 10110100:573 01001100:2 10110001:88 11101110:54 11101111:13 10111100:960 10111010:30 10111110:42 11001100:32 11001101:8 00111100:875 00111101:220 10011110:444 00011111:42 10110010:12 10110101:129 00101111:2 00101101:34 01010111:2 10101101:75 00101001:22 11011101:400 }\n",
      "\n",
      "{ 00011111:37 10110010:16 11101111:12 11101110:68 10110111:5 00010111:30 10100111:2 01010101:74 11111011:561 10110001:80 01001111:1 01001100:6 10110100:515 11011011:7 00010110:91 00000011:3 00111111:10 01101110:19 01101011:3 10011010:275 10111110:55 11001100:24 11001101:6 11111111:915 10110110:24 01101010:11 01111111:854 11100110:31 10110011:7 10110000:322 01011111:8 10111111:7 10111001:163 10101111:7 11011001:224 00011110:161 01000000:1 11110011:325 10111100:909 10111010:19 00111110:28 10110101:170 00111100:862 00111101:235 10011110:412 00001111:6 10100101:45 10111101:255 10001111:48 10111000:526 00101101:35 01010111:2 10101101:92 00101001:22 11011101:412 00001011:3 01101111:4 11110111:584 01011101:142 10011011:72 11011111:18 10111011:11 10011111:110 00000111:7 11100111:11 }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cudaq\n",
    "\n",
    "cudaq.set_target(\"nvidia-mqpu\")\n",
    "\n",
    "target = cudaq.get_target()\n",
    "qpu_count = target.num_qpus()\n",
    "print(\"Number of QPUs:\", qpu_count)\n",
    "\n",
    "@cudaq.kernel\n",
    "def qrbm(v_nodes:int, h_nodes:int, ancilla:int, theta: list[float], coupling: list[float]):\n",
    "\n",
    "    qubits_num=v_nodes+h_nodes+ancilla\n",
    "    qubits=cudaq.qvector(qubits_num)\n",
    "\n",
    "    for i in range(v_nodes+h_nodes):\n",
    "        ry(theta[i],qubits[i])\n",
    "\n",
    "    a_target=v_nodes+h_nodes\n",
    "    count=0\n",
    "    for v in range(v_nodes):\n",
    "        for h in range(v_nodes,v_nodes+h_nodes):\n",
    "            ry.ctrl(coupling[count],qubits[v],qubits[h],qubits[a_target])\n",
    "            x(qubits[v])\n",
    "            ry.ctrl(coupling[count+1],qubits[v],qubits[h],qubits[a_target])\n",
    "            x(qubits[v])\n",
    "            x(qubits[h])\n",
    "            ry.ctrl(coupling[count+1],qubits[v],qubits[h],qubits[a_target])\n",
    "            x(qubits[v])\n",
    "            ry.ctrl(coupling[count],qubits[v],qubits[h],qubits[a_target])\n",
    "            x(qubits[v])\n",
    "            x(qubits[h])\n",
    "\n",
    "            count+=2\n",
    "            a_target+=1\n",
    "\n",
    "    mz(qubits)    \n",
    "    \n",
    "v_nodes=2\n",
    "h_nodes=2\n",
    "ancilla=4\n",
    "\n",
    "# Initialize the parameters for the RBM\n",
    "theta=[2.0482, 1.4329, 2.1774, 2.7122]\n",
    "coupling=[1.8256, 3.1415, 1.8257, 3.1415, 3.1415, 0.4152, 3.1415, 0.9654]\n",
    "\n",
    "count_futures = []\n",
    "\n",
    "for qpu in range(3):\n",
    "    count_futures.append(cudaq.sample_async(qrbm,v_nodes, h_nodes, ancilla, theta, coupling, shots_count=10000,qpu_id=qpu))\n",
    "\n",
    "for counts in count_futures:\n",
    "    print(counts.get())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example with `observe` algorithmic primitives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of QPUs: 5\n",
      "Parameter shape:  (500, 10)\n",
      "Elapsed time (s) for single GPU:  1.8345087748020887\n",
      "We have 500 parameters which we would like to execute\n",
      "We split this into 4 batches of 125 , 125 , 125 , 125\n",
      "Shape after splitting (125, 10)\n",
      "Elapsed time (s) for multi-GPU:  0.006069962866604328\n"
     ]
    }
   ],
   "source": [
    "import cudaq\n",
    "from cudaq import spin\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "cudaq.set_target(\"nvidia-mqpu\")\n",
    "target = cudaq.get_target()\n",
    "qpu_count = target.num_qpus()\n",
    "print(\"Number of QPUs:\", qpu_count)\n",
    "\n",
    "qubit_count = 10\n",
    "sample_count = 500\n",
    "\n",
    "ham = spin.z(0)\n",
    "\n",
    "parameter_count = qubit_count\n",
    "\n",
    "# Below we run a circuit for 500 different input parameters.\n",
    "parameters = np.random.default_rng(13).uniform(low=0,high=1,size=(sample_count,parameter_count))\n",
    "\n",
    "print('Parameter shape: ', parameters.shape)\n",
    "\n",
    "@cudaq.kernel\n",
    "def kernel_rx(theta:list[float]):\n",
    "    qubits = cudaq.qvector(qubit_count)\n",
    "\n",
    "    for i in range(qubit_count):\n",
    "        rx(theta[i], qubits[i])\n",
    "\n",
    "#single GPU\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "result = cudaq.observe(kernel_rx, ham, parameters)\n",
    "energies = np.array([r.expectation() for r in result])\n",
    "\n",
    "end_time = timeit.default_timer()\n",
    "print('Elapsed time (s) for single GPU: ', end_time-start_time)\n",
    "\n",
    "#print('Energies from single GPU')\n",
    "#print(energies)\n",
    "\n",
    "\n",
    "# Multi-GPU\n",
    "\n",
    "# We split our parameters into 4 arrays since we have 4 GPUs available.\n",
    "xi = np.split(parameters,4)\n",
    "\n",
    "print('We have', parameters.shape[0],\n",
    "      'parameters which we would like to execute')\n",
    "\n",
    "print('We split this into', len(xi), 'batches of', xi[0].shape[0], ',',\n",
    "      xi[1].shape[0], ',', xi[2].shape[0], ',', xi[3].shape[0])\n",
    "\n",
    "print('Shape after splitting', xi[0].shape)\n",
    "asyncresults = []\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "for i in range(len(xi)):\n",
    "    for j in range(xi[i].shape[0]):\n",
    "        asyncresults.append(\n",
    "            cudaq.observe_async(kernel_rx, ham, xi[i][j, :], qpu_id=i))\n",
    "        \n",
    "end_time = timeit.default_timer()\n",
    "print('Elapsed time (s) for multi-GPU: ', end_time-start_time)\n",
    "\n",
    "#print('Energies from multi-GPUs')\n",
    "for result in asyncresults:\n",
    "    observe_result = result.get()\n",
    "    got_expectation = observe_result.expectation()\n",
    "    #print(got_expectation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Multi-GPU (nvidia-mgpu)\n",
    "\n",
    "The `nvidia-mgpu` backend is useful for running a large single quantum circuit spread across multiple GPUs.\n",
    "- A $n$ qubit quantum state has $2^n$ complex amplitudes, each of which require 8 bytes of memory to store. Hence the total memory required to store a n qubit quantum state is $8$ bytes $\\times 2^n$. For $n=30$ qubits, this is roughly $8$ GB but for $n=40$, this exponentially increases to $8700$ GB.\n",
    "\n",
    "#### Example: GHZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# mpirun -np 4 python <fname> --target nvidia-mgpu\n",
    "\n",
    "import cudaq\n",
    "\n",
    "cudaq.mpi.initialize()\n",
    "\n",
    "qubit_count = 33\n",
    "\n",
    "@cudaq.kernel\n",
    "def kernel(qubit_num: int):\n",
    "    # Allocate our qubits.\n",
    "    qvector = cudaq.qvector(qubit_num)\n",
    "    # Place the first qubit in the superposition state.\n",
    "    h(qvector[0])\n",
    "    # Loop through the allocated qubits and apply controlled-X,\n",
    "    # or CNOT, operations between them.\n",
    "    for qubit in range(qubit_num - 1):\n",
    "        x.ctrl(qvector[qubit], qvector[qubit + 1])\n",
    "    # Measure the qubits.\n",
    "    mz(qvector)\n",
    "\n",
    "#print(\"Preparing GHZ state for\", qubit_count, \"qubits.\")\n",
    "counts = cudaq.sample(kernel, qubit_count)\n",
    "\n",
    "if cudaq.mpi.rank() == 0:\n",
    "    print(counts)\n",
    "\n",
    "cudaq.mpi.finalize()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
